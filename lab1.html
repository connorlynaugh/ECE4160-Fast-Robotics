<!DOCTYPE HTML>
<!--
	Twenty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Lab 1 - ECE4160 Fast Robotics Spring 2026</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

	<style>
	:root{
	/* Media sizing: full width again */
	--media-min: 0px;
	--media-ideal: 100%;
	--media-max: 100%;

	--media-gap: 1.25rem;
	--media-bottom: 2.25rem;
	--section-gap: 3rem;
	}

	/* Inline code look */
	code.inline {
		background: rgba(0,0,0,0.06);
		padding: 0.1em 0.3em;
		border-radius: 4px;
	}

	/* ====== PROFESSIONAL HEADERS ====== */
	#main .content h3 {
		font-size: 2.0em;
		font-weight: 800;
		letter-spacing: 0.02em;
		margin-top: var(--section-gap);
		margin-bottom: 0.75rem;
	}

	#main .content h4 {
		font-size: 1.35em;
		font-weight: 750;
		margin-top: 2.25rem;
		margin-bottom: 0.6rem;
	}

	/* Better anchor-jump positioning (so headers aren’t hidden under top nav) */
	#main .content h3[id],
	#main .content h4[id] {
		scroll-margin-top: 95px;
	}

	/* Make section separators breathe more */
	#main .content hr {
		margin: 3rem 0;
	}

	/* ====== SMALLER, NICER IMAGES ====== */
	#main .content span.image.fit{
	display:block;
	width:100%;
	max-width: var(--media-max);
	margin: var(--media-gap) auto var(--media-bottom) auto;
	}

	#main .content span.image.fit img {
		width: 100%;
		height: auto;
	}

	/* Extra spacing if images are back-to-back */
	#main .content span.image.fit + span.image.fit {
		margin-top: calc(var(--media-gap) * 1.4);
	}

	/* ====== SMALLER, NICER VIDEOS ====== */
	.video-container{
	position: relative;
	width: 100%;
	max-width: var(--media-max);
	margin: var(--media-gap) auto var(--media-bottom) auto;
	padding-bottom: 56.25%;
	height: 0;
	overflow: hidden;
	border-radius: 10px;
	}

	.video-container iframe {
		position: absolute;
		top: 0; left: 0;
		width: 100%;
		height: 100%;
		border: 0;
	}

	/* Extra spacing if a video is followed by an image (or another video) */
	.video-container + .video-container,
	.video-container + span.image.fit,
	span.image.fit + .video-container {
		margin-top: calc(var(--media-gap) * 1.4);
	}

	/* Optional: use this class when you WANT a larger image/video */
	.media-wide {
		max-width: 900px !important;
	}
	</style>

	</head>

	<body class="no-sidebar is-preload">
		<div id="page-wrapper">

			<header id="header">
				<h1 id="logo"><a href="index.html">Fast Robotics <span>Spring 2026</span></a></h1>
				<nav id="nav">
					<ul>
						<li><a href="index.html">Welcome</a></li>
						<li class="current"><a href="lab1.html">Lab 1</a></li>
					</ul>
				</nav>
			</header>

			<article id="main">

				<header class="special container">
					<span class="icon solid fa-bluetooth-b"></span>
					<h2><strong>Lab 1: The Artemis Board and Bluetooth</strong></h2>
					<p>
						ECE4160 Fast Robotics — Spring 2026<br />
						Connor Lynaugh
					</p>
				</header>

				<section class="wrapper style4 container">
					<div class="content">
						<section>

							<section class="box">
								<header>
									<h3>Table of Contents</h3>
								</header>
								<ul class="alt">
									<li><a class="scrolly" href="#overview">Overview</a></li>

									<li>
										<a class="scrolly" href="#lab1a">Lab 1A</a>
										<ul style="margin: 0.5em 0 0 1.25em;">
											<li><a class="scrolly" href="#prelab1a">Prelab 1A</a></li>
											<li><a class="scrolly" href="#blink">Blink</a></li>
											<li><a class="scrolly" href="#serial">Serial</a></li>
											<li><a class="scrolly" href="#temperature">Temperature</a></li>
											<li><a class="scrolly" href="#microphone">Microphone</a></li>
										</ul>
									</li>

									<li>
										<a class="scrolly" href="#lab1b">Lab 1B</a>
										<ul style="margin: 0.5em 0 0 1.25em;">
											<li><a class="scrolly" href="#prelab1b">Prelab 1B</a></li>
											<li><a class="scrolly" href="#labtasks">Lab Tasks</a></li>
										</ul>
									</li>

									<li><a class="scrolly" href="#reflection">Reflection</a></li>
									<li><a class="scrolly" href="#acknowledgements">Acknowledgements</a></li>
								</ul>
							</section>

							<hr />

							<header>
								<h3 id="overview">Overview</h3>
							</header>
							<p>
								This lab was the first setup stage for the Sparkfun Artemis. This included programming the SparkFun RedBoard Artemis Nano
								in the Arduino IDE and then building a reliable BLE “debug channel” between the Artemis and my laptop
								using the provided Python/Jupyter framework. By the end of the lab I could run basic board examples, send structured commands from Python to the Artemis, and stream timestamped and temperature-tagged
								data back over BLE in a format that is easy to parse.
							</p>

							<hr />

							<header>
								<h3 id="lab1a">Lab 1A</h3>
							</header>

							<h4 id="prelab1a">Prelab 1A</h4>
							<p>
								I installed the Arduino IDE and added the SparkFun Apollo3 board package so the IDE could recognize the
								RedBoard Artemis Nano. On macOS I initially ran into a bootloader/serial upload issue, which ended up being
								a driver problem. After updating the CH340 driver and confirming the correct board and port selection, uploads
								and serial output worked consistently. From that point onward I kept my serial baud rate aligned with the
								examples and verified each step using serial monitor output and simple visible behaviors (like the onboard LED).
							</p>

							<hr />

							<h4 id="blink">Blink</h4>
							<p>
								I started with the standard Blink example to confirm that the toolchain was actually
								programming the board and that I could control the onboard LED. This was my “sanity check” before moving on to
								sensor and communication examples.
							</p>

							<p>
								The youtube reel displays this properly flashing the onboard LED.
							</p>

							<!-- Blink Short -->
							<div class="video-container">
								<iframe
									src="https://www.youtube-nocookie.com/embed/-seWNS9Mi8M?rel=0&modestbranding=1&playsinline=1"
									title="Blink Demo"
									allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
									referrerpolicy="strict-origin-when-cross-origin"
									allowfullscreen></iframe>
							</div>

							<h4 id="serial">Serial</h4>
							<p>
								Next I ran SparkFun’s serial example to verify two-way USB serial communication. The key detail for my setup was
								using 115200 baud so the monitor output was readable and input echo behavior matched
								expectations. Once this was working, it became my main debugging tool while I iterated on BLE in Lab 1B.
							</p>

							<span class="image fit">
								<img src="images/lab1/Echo.png" alt="Blink / LED example evidence" />
							</span>

							<h4 id="temperature">Temperature</h4>
							<p>
								I then tested the ADC-based temperature reading example. The goal here was not absolute accuracy but verifying that
								the board could sample a real sensor and that readings changed with a physical stimulus. Touching the chip and waiting a
								moment produced a noticeable increase in measured temperature, which confirmed the sensor was behaving as expected.
							</p>

							<span class="image fit">
								<img src="images/lab1/Temperatures.png" alt="Serial monitor output / echo behavior" />
							</span>

							<h4 id="microphone">Microphone</h4>
							<p>
								Finally I ran the Pulse Density Modulation (PDM) microphone output example to confirm microphone sampling and frequency content extraction.
								When I spoke or whistled near the board, the dominant frequency content shifted upward. This also gave me confidence that higher-rate streaming data would be feasible later
								in the course.
							</p>

							<p>
								This youtube reel displays this how the frequency content changes while speaking.
							</p>

							<div class="video-container">
								<iframe
									src="https://www.youtube-nocookie.com/embed/Ue5I-JnAfjk?rel=0&modestbranding=1&playsinline=1"
									title="Microphone Demo"
									allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
									referrerpolicy="strict-origin-when-cross-origin"
									allowfullscreen></iframe>
							</div>

							<hr />

							<header>
								<h3 id="lab1b">Lab 1B</h3>
							</header>

							<h4 id="prelab1b">Prelab 1B</h4>
							<p>
								For BLE, I set up a dedicated Python virtual environment and ran the course Jupyter notebook workflow.
								It is organized such that Python writes a command string to a writable RX characteristic, the Artemis
								parses it and responds by updating a TX string.
							</p>
							<p>
								In my Arduino sketch, I changed the custom service UUID. On the Python side, the controller loads the MAC address and UUIDs
								from connections.yaml.
							</p>

							<span class="image fit">
								<img src="images/lab1/Mac.png" alt="MAC address / connections config evidence" />
							</span>

							<p>
								I also generated a unique BLE service UUID to avoid accidentally connecting to a nearby board advertising the default UUID.
							</p>
							<p>
								The generated UUID: "772587a5-ad3b-4905-a21a-ee5d1d3ee220"
							</p>

							<hr />

							<header>
								<h3 id="labtasks">Lab Tasks</h3>
							</header>

							<h4 id="echo">1. Echo</h4>
							<p>
								I implemented ECHO by reading a string argument from the command payload, then sending back an
								augmented response through the notify TX string characteristic. In my implementation I used the provided EString
								helper so I could safely build a C-string without manual buffer management.
							</p>
							<span class="image fit">
								<img src="images/lab1/EchoArduino.png" alt="Echo Arduino case" />
							</span>
							<p>
								Here the arduino added "Robot says ->" and ":)".
							</p>
							<span class="image fit">
								<img src="images/lab1/Hello.png" alt="Echo Python output" />
							</span>

							<h4 id="send-three-floats">2. Send Three Floats</h4>
							<p>
								For SEND_THREE_FLOATS, I extended the parsing pattern used in the course’s two-integer example.
								The Artemis uses RobotCommand.get_next_value() to extract three float values in sequence and then
								prints them to the serial monitor for verification.
							</p>

							<span class="image fit">
								<img src="images/lab1/send3Arduino.png" alt="Send three floats Arduino" />
							</span>
							<p>
								Here I send 6.0 7.0 and 8.0 and the respective results are shown below.
							</p>
							<span class="image fit">
								<img src="images/lab1/send3Python.png" alt="Send three floats Python" />
							</span>
							<span class="image fit">
								<img src="images/lab1/send3Result.png" alt="Send three floats results" />
							</span>

							<h4 id="get-time-millis">3. Get Time Millis</h4>
							<p>
								I added GET_TIME_MILLIS to return a timestamp formatted as T:&lt;millis&gt;
								I intentionally kept the payload short and easy to parse on the Python side.
							</p>

							<span class="image fit">
								<img src="images/lab1/GetTimeArduino.png" alt="GET_TIME_MILLIS Arduino" />
							</span>
							<p>
								This yielded the proper response show in Jupyter.
							</p>
							<span class="image fit">
								<img src="images/lab1/GetTimePython.png" alt="GET_TIME_MILLIS Python" />
							</span>

							<h4 id="notification-handler">4. Notification Handler</h4>
							<p>
								On the Python side I used a notification callback in ble.py to asynchronously receive TX string updates. I simply splice the passed string and add the time values to a growing array.
							</p>

							<span class="image fit">
								<img src="images/lab1/NotificationHandler.png" alt="Notification handler code" />
							</span>
							<p>
								I later updated this to include temperature data and possibly more iterable data in the future. For temperature packets,
								my Arduino sends strings formatted as T:&lt;ms&gt; F:&lt;degF&gt;, so the handler splits at the first space
								and stores time and temperature into separate lists.
							</p>
							<span class="image fit">
								<img src="images/lab1/UpdatedTimeHandler.png" alt="Updated handler parsing" />
							</span>

							<h4 id="time-loop">5. TIME_LOOP</h4>
							<p>
								TIME_LOOP repeatedly publishes timestamps for 5 seconds using BLE notifications.
								I used the number of notifications received over that window to estimate the maximum sustained message rate.
								My measured rate was 24 messages/sec.
							</p>

							<span class="image fit">
								<img src="images/lab1/TimeLoopArduino.png" alt="TIME_LOOP Arduino" />
							</span>
							<span class="image fit">
								<img src="images/lab1/TimeLoopPython.png" alt="TIME_LOOP Python results" />
							</span>

							<h4 id="send-time-data">6. SEND_TIME_DATA</h4>
							<p>
								In SEND_TIME_DATA, I switched from measure and transmit immediately to measure locally, then transmit.
								The Artemis stores timestamps into a fixed-size global array (length 500), then transmits the buffered samples back to the laptop.
								This pattern is useful when you want sampling to be less coupled to BLE latency.
							</p>

							<span class="image fit">
								<img src="images/lab1/SendTimeDataArduino.png" alt="SEND_TIME_DATA Arduino" />
							</span>
							<span class="image fit">
								<img src="images/lab1/SendTimeDataPython.png" alt="SEND_TIME_DATA Python" />
							</span>

							<h4 id="get-temp-readings">7. GET_TEMP_READINGS</h4>
							<p>
								Finally, GET_TEMP_READINGS records paired samples of time and temperature into two aligned arrays.
								Each index corresponds to a single measurement instant. After the capture window, the Artemis sends strings back in the format
								T:&lt;ms&gt; F:&lt;degF&gt;, which the Python handler parses into times and temps lists.
							</p>

							<span class="image fit">
								<img src="images/lab1/GetTempArduino.png" alt="GET_TEMP_READINGS Arduino" />
							</span>
							<span class="image fit">
								<img src="images/lab1/GetTempPython.png" alt="GET_TEMP_READINGS Python" />
							</span>
							<span class="image fit">
								<img src="images/lab1/GetTempOutput.png" alt="GET_TEMP_READINGS output" />
							</span>

							<h4 id="lab-question">8. Discussion: incremental vs buffered sampling</h4>
							<p>
								The “incremental” approach (requesting GET_TIME_MILLIS repeatedly) is simple and gives immediate feedback,
								which is helpful while debugging. The downside is that BLE round-trip time dominates the sampling interval, so the effective
								sampling rate is relatively low. This is not ture on the flipside where all measurements are done upfront.
							</p>
							<p>
								The buffered approach (recording arrays locally, then sending the array) seperates sampling speed from BLE latency.
								Sampling in a tight loop on the Artemis can be much faster and more consistent, but it costs RAM and delays feedback until the array is sent.
								In my implementation, arrays are fixed-size (500 samples), which keeps memory use predictable.
							</p>
							<p>
								For memory bounds: the Artemis has 384 kB RAM (~393,216 bytes). A timestamp-only buffer using 4-byte integers can store roughly
								393,216 / 4 ≈ 98,304 samples. If I store both timestamp (4 bytes) and temperature float (4 bytes), the per-sample storage is ~8 bytes,
								so the upper bound is about 49,152 paired samples (ignoring overhead from other variables and stack usage).
							</p>

							<hr />

							<header>
								<h3 id="reflection">Reflection</h3>
							</header>
							<p>
								The biggest lesson from this lab was that “communications working” is not simple. Getting a connection is one problem but getting
								reliable, parseable data at a useful rate is another. Most of my time went into small integration details (drivers, baud rate, matching UUIDs,
								and making sure my Bluetooth was actually enabled). Once the pipeline was stable, the command-based architecture (RX command string +
								TX notify string) made it straightforward to add new commands for future labs.
							</p>

							<hr />

							<header>
								<h3 id="acknowledgements">Acknowledgements</h3>
							</header>
							<p>
								Thank you to the course staff for help during setup and debugging. I also referenced a few prior student pages to understand how
								to present results clearly (Katarina Duric and Nita Kattimani). The use of ChatGPT was used to leverage html formatting including text, images, and video. AI also did a grammar check on my report since I did not type it up online.
							</p>

							<ul class="buttons">
								<li><a href="#page-wrapper" class="button small scrolly">Back to Top</a></li>
								<li><a href="index.html" class="button small">Back to Lab Reports</a></li>
							</ul>

						</section>
					</div>
				</section>

			</article>

			<footer id="footer">
				<ul class="copyright">
					<li>&copy; Connor Lynaugh</li>
					<li>Design: HTML5 UP</li>
				</ul>
			</footer>

		</div>

		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.dropotron.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/jquery.scrollex.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

	</body>
</html>
